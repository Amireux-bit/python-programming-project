from pydantic import BaseModel, Field
from tools.base import BaseTool
from .utils import monitor_execution
from functools import lru_cache
import json
import requests
import re
import os
import numpy as np

# === å°è¯•å¯¼å…¥ AI åº“ (å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œä»£ç ä¼šè‡ªåŠ¨é™çº§ä¸ºçº¯ Google æœç´¢) ===
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    HAS_LOCAL_SEARCH = True
except ImportError:
    HAS_LOCAL_SEARCH = False
    print("âš ï¸ Warning: sentence-transformers or faiss not installed. Local search disabled.")


# === 1. æœ¬åœ°çŸ¥è¯†åº“å•ä¾‹ (æ”¯æŒæŒä¹…åŒ–ç¼“å­˜) ===
class LocalKnowledgeBase:
    _instance = None

    def __new__(cls, doc_dir="data/docs"):
        if cls._instance is None:
            cls._instance = super(LocalKnowledgeBase, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def init_kb(self, doc_dir="data/docs"):
        if self.initialized or not HAS_LOCAL_SEARCH: return
        
        # å®šä¹‰ç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_dir = "data/cache"
        if not os.path.exists(cache_dir): os.makedirs(cache_dir)
        
        index_path = os.path.join(cache_dir, "faiss_index.bin")
        docs_path = os.path.join(cache_dir, "documents.json")

        # ğŸŸ¢ ç­–ç•¥ A: å°è¯•ä»ç¼“å­˜åŠ è½½ (æé€Ÿæ¨¡å¼)
        if os.path.exists(index_path) and os.path.exists(docs_path):
            print("ğŸš€ [SmartSearch] Found cache! Loading from disk...")
            try:
                # 1. åŠ è½½ FAISS ç´¢å¼•
                self.index = faiss.read_index(index_path)
                # 2. åŠ è½½æ–‡æœ¬æ•°æ®
                with open(docs_path, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                
                # 3. å¿…é¡»åŠ è½½æ¨¡å‹ä»¥ä¾¿åç»­æŠŠ Query è½¬å‘é‡ (ä½†ä¸éœ€è¦é‡æ–° Embed æ–‡æ¡£äº†)
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                
                print(f"âœ… [SmartSearch] Cache loaded. Ready with {len(self.documents)} documents.")
                self.initialized = True
                return
            except Exception as e:
                print(f"âš ï¸ [SmartSearch] Cache corrupted ({e}), rebuilding...")

        # ğŸŸ  ç­–ç•¥ B: ç¼“å­˜ä¸å­˜åœ¨ï¼Œé‡æ–°æ„å»º (æ…¢é€Ÿæ¨¡å¼)
        print("ğŸ“¥ [SmartSearch] Building Knowledge Base from scratch...")
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.documents = []
        
        if not os.path.exists(doc_dir):
            os.makedirs(doc_dir)
            with open(f"{doc_dir}/readme.txt", "w") as f: 
                f.write("Place your txt files here.")

        for filename in os.listdir(doc_dir):
            if filename.endswith(".txt"):
                file_path = os.path.join(doc_dir, filename)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        chunks = [c.strip() for c in content.split('\n\n') if len(c.strip()) > 30]
                        self.documents.extend([(c, filename) for c in chunks])
                except Exception as e:
                    print(f"Error reading {filename}: {e}")

        if self.documents:
            # å‘é‡åŒ–
            texts = [doc[0] for doc in self.documents]
            print(f"â³ Embedding {len(texts)} snippets... (This may take a while)")
            embeddings = self.model.encode(texts)
            
            # å»ºç´¢å¼•
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dimension)
            self.index.add(np.array(embeddings).astype('float32'))
            
            # ğŸ’¾ ä¿å­˜ç¼“å­˜ (å…³é”®æ­¥éª¤)
            print("ğŸ’¾ Saving cache to disk...")
            faiss.write_index(self.index, index_path)
            with open(docs_path, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f)
                
            print(f"âœ… [SmartSearch] Built and saved {len(self.documents)} snippets.")
        else:
            print("âš ï¸ [SmartSearch] No documents found.")
        
        self.initialized = True

    # ... (search æ–¹æ³•ä¿æŒä¸å˜) ...
    def search(self, query, top_k=2):
        # ä¿æŒåŸæ ·
        if not self.initialized or not self.documents: return [], []
        query_vec = self.model.encode([query])
        distances, indices = self.index.search(np.array(query_vec).astype('float32'), top_k)
        results = []
        scores = []
        for i, idx in enumerate(indices[0]):
            if idx != -1:
                results.append(self.documents[idx])
                scores.append(distances[0][i])
        return results, scores
    
    
# åˆå§‹åŒ–å…¨å±€ KB å®ä¾‹
kb = LocalKnowledgeBase()
# å»ºè®®åœ¨ main.py å¯åŠ¨æ—¶è°ƒç”¨ kb.init_kb()ï¼Œä½†è¿™é‡Œä¸ºäº†é²æ£’æ€§ï¼Œä¼šåœ¨ run é‡Œæ‡’åŠ è½½

# === 2. æ··åˆæœç´¢å·¥å…· ===
class SearchArgs(BaseModel):
    query: str = Field(..., description="The query string to search for.")

class SmartSearchTool(BaseTool):
    name = "search" # ä¿æŒåå­—å« searchï¼Œè¿™æ ·é˜Ÿå‹çš„ prompt ä¸ç”¨æ”¹
    description = (
        "Intelligent Search Tool. "
        "First checks the local verified knowledge base (policies, guides). "
        "If no match found, searches the live internet using Google. "
        "Returns structured data with confidence scores."
    )
    args_schema = SearchArgs
    
    # ä½ çš„ API Key
    api_key: str = "8c5a260710c9c2aceda64505b3a551d88a7a14b6" 

    def _calculate_google_confidence(self, query: str, snippet: str, rank: int) -> float:
        """
        ä½ çš„æ··åˆç½®ä¿¡åº¦ç®—æ³• (ä¿ç•™åŸæ ·)
        """
        rank_score = max(0.5, 0.9 - (rank * 0.05))
        q_tokens = set(re.findall(r'\w+', query.lower()))
        s_tokens = set(re.findall(r'\w+', snippet.lower()))
        if not q_tokens: return rank_score
        overlap = len(q_tokens.intersection(s_tokens))
        match_ratio = overlap / len(q_tokens)
        final_score = (rank_score * 0.7) + (match_ratio * 0.3)
        return round(final_score, 2)

    def _search_google(self, query: str) -> list:
        """
        çº¯ Google æœç´¢é€»è¾‘
        """
        url = "https://google.serper.dev/search"
        payload = json.dumps({"q": query, "num": 5})
        headers = {'X-API-KEY': self.api_key, 'Content-Type': 'application/json'}

        results = []
        try:
            response = requests.request("POST", url, headers=headers, data=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if "organic" in data:
                    for index, item in enumerate(data["organic"]):
                        snippet = item.get("snippet", "")
                        # è°ƒç”¨ä½ çš„ç®—æ³•
                        confidence = self._calculate_google_confidence(query, snippet, index)
                        results.append({
                            "content": snippet,
                            "source": item.get("link", ""),
                            "title": item.get("title", ""),
                            "score": confidence,
                            "type": "internet" # æ ‡è®°æ¥æº
                        })
        except Exception as e:
            print(f"Google Search Error: {e}")
        return results

    @monitor_execution(tool_name="smart_search")
    @lru_cache(maxsize=50) 
    def run(self, query: str) -> str:
        kb.init_kb()
        final_results = []
        
        # é˜ˆå€¼ä¿æŒä¸å˜
        LOCAL_THRESHOLD = 0.95 
        
        # ğŸ”´ æ”¹è¿›ç‚¹ 1: è·å–æ›´å¤šå€™é€‰ (æ¯”å¦‚ Top 3)
        local_docs, local_dists = kb.search(query, top_k=3)
        
        found_local_match = False
        valid_local_snippets = [] # ç”¨æ¥å­˜æ‰€æœ‰åˆæ ¼çš„ç‰‡æ®µ
        
        if local_docs:
            # åªè¦ç¬¬ä¸€æ¡åˆæ ¼ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå‘½ä¸­äº†æœ¬åœ°
            if local_dists[0] < LOCAL_THRESHOLD:
                found_local_match = True
                
                # ğŸ”´ æ”¹è¿›ç‚¹ 2: æŠŠæ‰€æœ‰åˆæ ¼çš„ç‰‡æ®µéƒ½æ”¶é›†èµ·æ¥
                for (doc_content, filename), dist in zip(local_docs, local_dists):
                    if dist < LOCAL_THRESHOLD:
                        valid_local_snippets.append(f"--- (Source: {filename}) ---\n{doc_content}")

        # å¦‚æœå‘½ä¸­äº†æœ¬åœ°ï¼ŒæŠŠæ”¶é›†åˆ°çš„ç‰‡æ®µæ‹¼æˆä¸€ä¸ªå¤§ç»“æœè¿”å›
        if found_local_match:
            # ç”¨æ¢è¡Œç¬¦æ‹¼æ¥
            combined_content = "\n\n".join(valid_local_snippets)
            final_results.append({
                "content": f"[Verified Local Guide]:\n{combined_content}",
                "source": "Local DB (Multiple Hits)", 
                "title": "Local Knowledge Base Match",
                "score": 1.0, 
                "type": "local"
            })

        # è”ç½‘éƒ¨åˆ†ä¿æŒä¸å˜
        if not found_local_match:
            google_results = self._search_google(query)
            final_results.extend(google_results)
            
        final_results.sort(key=lambda x: x["score"], reverse=True)
        
        if not final_results:
            return json.dumps({"status": "EMPTY", "results": []})

        return json.dumps({
            "status": "SUCCESS", 
            "results": final_results
        })